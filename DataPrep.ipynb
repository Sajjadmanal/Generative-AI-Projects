{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2654189b",
   "metadata": {},
   "source": [
    "## High level steps\n",
    "\n",
    "As part of the session we will perform following steps to perform data preparation\n",
    "\n",
    "1. Downloading our IMDB dataset\n",
    "2. Will perform text processing\n",
    "3. Generate the embeddings\n",
    "4. Create and store to Index\n",
    "5. Call GPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54845ba",
   "metadata": {
    "gather": {
     "logged": 1700673681852
    }
   },
   "outputs": [],
   "source": [
    "#Pandas library to manage dataframes\n",
    "import pandas as pd\n",
    "#import numpy for plotting\n",
    "import numpy as np\n",
    "#Import the stop words library\n",
    "from nltk.corpus import stopwords\n",
    "#Import the nltk library\n",
    "import nltk\n",
    "#library for regular expression\n",
    "import re\n",
    "#library for parsing html\n",
    "from bs4 import BeautifulSoup\n",
    "#library for progress bar\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3474f622",
   "metadata": {},
   "source": [
    "### Step 1: Download the IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee77cc7",
   "metadata": {
    "gather": {
     "logged": 1700673682265
    }
   },
   "outputs": [],
   "source": [
    "#specify path of file\n",
    "url=\"https://raw.githubusercontent.com/SrinidhiRaghavan/AI-Sentiment-Analysis-on-IMDB-Dataset/master/imdb_tr.csv\"\n",
    "# Reading a file\n",
    "imdb_data=pd.read_csv(url,encoding='latin-1') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a30581",
   "metadata": {
    "gather": {
     "logged": 1700673682548
    }
   },
   "outputs": [],
   "source": [
    "#getting the top 10 records\n",
    "imdb_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b063e42a",
   "metadata": {},
   "source": [
    "### Step 2: Will perform text processing\n",
    "- As part of text pre processing we will be expanding the words like won't to will not\n",
    "- Changing the case to lower case\n",
    "- Stripping all http addresses\n",
    "- Extracting text from HTML tag\n",
    "- Stripping sentence from all digits and other characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76e0635",
   "metadata": {
    "gather": {
     "logged": 1700673682805
    }
   },
   "outputs": [],
   "source": [
    "#downloading the stop words\n",
    "nltk.download('stopwords')\n",
    "#We will set stop words that we plan to use as english\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fd0552",
   "metadata": {
    "gather": {
     "logged": 1700673682978
    }
   },
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c7ec1f",
   "metadata": {
    "gather": {
     "logged": 1700673683281
    }
   },
   "outputs": [],
   "source": [
    "def pre_process(data):\n",
    "    preprocessed_reviews = []\n",
    "    # tqdm is for printing the status bar\n",
    "    for sentance in tqdm(data):\n",
    "        sentance = sentance.lower()\n",
    "        #remove http address\n",
    "        sentance = re.sub(r\"http\\S+\", \"\", sentance)\n",
    "        #extracting text from html tags\n",
    "        sentance = BeautifulSoup(sentance, 'lxml').get_text()\n",
    "        #expanding the words\n",
    "        sentance = decontracted(sentance)\n",
    "        #remove all numbers\n",
    "        #sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n",
    "        #replacing all characters other than a-z with white space\n",
    "        #sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n",
    "        #removing stop words\n",
    "        #sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stop_words)\n",
    "        preprocessed_reviews.append(sentance.strip())\n",
    "    return preprocessed_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009fdccc",
   "metadata": {
    "gather": {
     "logged": 1700673690163
    }
   },
   "outputs": [],
   "source": [
    "#Call the prerpocess method\n",
    "preprocessed_reviews = pre_process(imdb_data['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bd5811",
   "metadata": {
    "gather": {
     "logged": 1700673690321
    }
   },
   "outputs": [],
   "source": [
    "#convert the extracted data into data frame\n",
    "preprocessed_imdb_data = pd.DataFrame({'preprocessed_text':preprocessed_reviews, 'polarity': imdb_data['polarity'] })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c659bb",
   "metadata": {
    "gather": {
     "logged": 1700673690557
    }
   },
   "outputs": [],
   "source": [
    "#Lets see the first few records\n",
    "preprocessed_imdb_data = preprocessed_imdb_data[:500]\n",
    "preprocessed_imdb_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbb7908",
   "metadata": {},
   "source": [
    "### Step 3: Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad69604",
   "metadata": {
    "gather": {
     "logged": 1700673690822
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries  \n",
    "import time\n",
    "import os  \n",
    "import json  \n",
    "import openai  \n",
    "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt  \n",
    "from azure.core.credentials import AzureKeyCredential  \n",
    "from azure.search.documents import SearchClient  \n",
    "from azure.search.documents.indexes import SearchIndexClient  \n",
    "from azure.search.documents.models import Vector  \n",
    "from azure.search.documents.indexes.models import (  \n",
    "    SearchIndex,  \n",
    "    SearchField,  \n",
    "    SearchFieldDataType,  \n",
    "    SimpleField,  \n",
    "    SearchableField,  \n",
    "    SearchIndex,  \n",
    "    SemanticConfiguration,  \n",
    "    PrioritizedFields,  \n",
    "    SemanticField,  \n",
    "    SearchField,  \n",
    "    SemanticSettings,  \n",
    "    VectorSearch,  \n",
    "    HnswVectorSearchAlgorithmConfiguration,  \n",
    ")  \n",
    "\n",
    "# models\n",
    "EMBEDDING_MODEL = \"text-embeddings-ada-002\"\n",
    "\n",
    "# Configure environment variables  \n",
    "service_endpoint = \"https://s360-azcognitivesearch-dev.search.windows.net\" \n",
    "openai.api_type = \"azure\"  \n",
    "openai.api_base = \"https://pwopenai.openai.azure.com\" \n",
    "openai.api_version = \"2023-05-15\"  \n",
    "\n",
    "cognitiveSearchKey = 'abc'\n",
    "credential = AzureKeyCredential(cognitiveSearchKey)\n",
    "openai.api_key = 'xyz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb54cc5-6a58-4c1c-807a-99b60f08e617",
   "metadata": {
    "gather": {
     "logged": 1700673748510
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def get_embeddings_with_retry(df, text_column, engine):\n",
    "    embeddings = []\n",
    "    for text in df[text_column]:\n",
    "        while True:\n",
    "            try:\n",
    "                embedding = get_embedding(text, engine=engine)\n",
    "                embeddings.append(embedding)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e.message}. Retrying after 1 second...\")\n",
    "                time.sleep(1)\n",
    "                continue\n",
    "    return embeddings\n",
    "\n",
    "preprocessed_imdb_data['textVector'] = get_embeddings_with_retry(preprocessed_imdb_data, 'preprocessed_text', EMBEDDING_MODEL)\n",
    "preprocessed_imdb_data['id'] =preprocessed_imdb_data.index.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1160c5-5233-4c71-acbf-3f2293e77c02",
   "metadata": {
    "gather": {
     "logged": 1700673748982
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "len(preprocessed_imdb_data[:1].textVector[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d99a2e-7b18-4381-909e-0bb0be4641b9",
   "metadata": {
    "gather": {
     "logged": 1700673749661
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "preprocessed_imdb_data.to_csv(\"preprocessed_imdb_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76596209-e4e5-466d-971f-e57970a8772e",
   "metadata": {
    "gather": {
     "logged": 1700673750080
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# preprocessed_imdb_data = pd.read_csv(\"preprocessed_imdb_data.csv\")\n",
    "# preprocessed_imdb_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbea247",
   "metadata": {
    "gather": {
     "logged": 1700673750501
    }
   },
   "outputs": [],
   "source": [
    "#check embeddings\n",
    "preprocessed_imdb_data['polarity'] = preprocessed_imdb_data['polarity'].astype(str).replace({'1': 'True', '0': 'False'})\n",
    "preprocessed_imdb_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa106507-2a05-4ee2-8204-02d50b879aa3",
   "metadata": {
    "gather": {
     "logged": 1700673750913
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "preprocessed_imdb_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e911e25",
   "metadata": {},
   "source": [
    "### Step 4: Create Search Index and store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73736cd2",
   "metadata": {
    "gather": {
     "logged": 1700673751360
    }
   },
   "outputs": [],
   "source": [
    "# Create a search index\n",
    "index_client = SearchIndexClient(\n",
    "    endpoint=service_endpoint, credential=credential)\n",
    "fields = [\n",
    "    SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True),\n",
    "    SearchableField(name=\"preprocessed_text\", type=SearchFieldDataType.String, filterable=True, retrievable=True, searchable=True),\n",
    "    SearchableField(name=\"polarity\", type=SearchFieldDataType.String, filterable=True, retrievable=True, searchable=True),\n",
    "    SearchField(name=\"textVector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                searchable=True, vector_search_dimensions=1536, vector_search_configuration=\"vector_config\"),\n",
    "]\n",
    "\n",
    "vector_search = VectorSearch(\n",
    "    algorithm_configurations=[\n",
    "        HnswVectorSearchAlgorithmConfiguration(\n",
    "            name=\"vector_config\",\n",
    "            kind=\"hnsw\",\n",
    "            parameters={\n",
    "                \"m\": 4,\n",
    "                \"efConstruction\": 400,\n",
    "                \"efSearch\": 500,\n",
    "                \"metric\": \"cosine\"\n",
    "            }\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "semantic_config = SemanticConfiguration(\n",
    "    name=\"semantic-config\",\n",
    "    prioritized_fields=PrioritizedFields(\n",
    "        title_field=SemanticField(field_name=\"preprocessed_text\"),\n",
    "        prioritized_keywords_fields=[SemanticField(field_name=\"preprocessed_text\"), SemanticField(field_name=\"polarity\")],\n",
    "        prioritized_content_fields=[SemanticField(field_name=\"preprocessed_text\"), SemanticField(field_name=\"polarity\")]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create the semantic settings with the configuration\n",
    "semantic_settings = SemanticSettings(configurations=[semantic_config])\n",
    "\n",
    "# Create the search index with the semantic settings\n",
    "index = SearchIndex(name=\"movie-index\", fields=fields,\n",
    "                    vector_search=vector_search, semantic_settings=semantic_settings)\n",
    "index_client.delete_index(index)\n",
    "result = index_client.create_or_update_index(index)\n",
    "print(f' {result.name} created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efee4915",
   "metadata": {
    "gather": {
     "logged": 1700673755433
    }
   },
   "outputs": [],
   "source": [
    "# Upload some documents to the index\n",
    "documents = preprocessed_imdb_data.to_dict(orient='records')  \n",
    "search_client = SearchClient(endpoint=service_endpoint, index_name=\"movie-index\", credential=credential)\n",
    "result = search_client.upload_documents(documents)  \n",
    "print(f\"Uploaded {len(documents)} documents\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdacceba",
   "metadata": {},
   "source": [
    "#### Test Hybrid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15015f2f-8610-4858-965e-ba27f1e9fd76",
   "metadata": {
    "gather": {
     "logged": 1700673755861
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# search function\n",
    "def strings_ranked_by_relatedness(\n",
    "    query: str,\n",
    "    top_n: int = 10\n",
    "):\n",
    "    \"\"\"Returns a list of strings and relatednesses, sorted from most related to least.\"\"\"\n",
    "    search_client = SearchClient(service_endpoint, index_name=\"movie-index\", credential=credential)\n",
    "    vector = Vector(value=get_embedding(query, engine=EMBEDDING_MODEL), k=10, fields=\"textVector\")\n",
    "  \n",
    "    results = search_client.search(  \n",
    "        search_text=query,  \n",
    "        vectors= [vector],\n",
    "        select=[\"preprocessed_text\", \"polarity\"],\n",
    "        query_type=\"semantic\", query_language=\"en-us\", semantic_configuration_name='semantic-config', query_caption=\"extractive\", query_answer=\"extractive\",\n",
    "    )  \n",
    "    content = []\n",
    "    for i, result in enumerate(results):\n",
    "        if i<top_n:\n",
    "            data = {}\n",
    "            data[\"Score\"] = result['@search.score']\n",
    "            data[\"Polarity\"] = result['polarity']\n",
    "            data[\"text\"] = result['preprocessed_text']\n",
    "            content.append(data)  \n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d10571e",
   "metadata": {},
   "source": [
    "### Step 5. Call GPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10a8770",
   "metadata": {
    "gather": {
     "logged": 1700673756285
    }
   },
   "outputs": [],
   "source": [
    "def ask(query):\n",
    "  lst = []\n",
    "  content= strings_ranked_by_relatedness(query, top_n=5)\n",
    "  completion = openai.ChatCompletion.create(\n",
    "    engine = \"gpt-35-turbo\",\n",
    "    temperature = 0.0,\n",
    "    max_tokens = 2000,\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a movie expert who uses the below data to answer questions. Do not reply anything outside the mentioned data.\"},\n",
    "        {\"role\": \"user\", \"content\": \" \".join([str(i) for i in content])+ \"\\n QUery: \" + query}]\n",
    "  )\n",
    "\n",
    "  print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08909ba",
   "metadata": {
    "gather": {
     "logged": 1700673759442
    }
   },
   "outputs": [],
   "source": [
    "# examples\n",
    "ask(\"Tell me some worst reviews by people.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a046edc0-b277-4f71-9798-58f556ff59df",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0777ba6f",
   "metadata": {
    "gather": {
     "logged": 1700668720157
    }
   },
   "outputs": [],
   "source": [
    "imdb_data = imdb_data[:500]\n",
    "imdb_data[imdb_data['text'].str.contains(\"Drew Barrymore\")].text.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c18332",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "0a2b795fc10205748df850303d21a60bba0570ef12e6dbc1ea99e0810b8a8ecf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
